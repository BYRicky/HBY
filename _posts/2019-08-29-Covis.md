---
layout: post
title: '(preprint) Visual Localization Using Sparse Semantic 3D Map'
subtitle: 'Authors: Tianxin Shi, Shuhan Shen, Xiang Gao, and Lingjie Zhu'
tags: ["visual localization", "point clouds", "semantic"]

---

## Paper
<a href="https://arxiv.org/abs/1904.03803"> Visual Localization Using Sparse Semantic 3D Map</a>

---

## Motivation
- most traditional mathods fails to locate the camera **under a wide range of viewing conditions variations** including season and illumination changes,as well as weather and day-night varitations.
  
---

## Proposed scheme
- combine image-based and structure-based localization with `semantic information`
- separate into three parts:
  - sparase semantic 3D map
    - apply off-the-shelf segmentation CNNs (DeepLabv3+ network) to all database images
    - reproject all database images to 3D point cloud 
    - apply maximum voting to allocate a labels for each 3D point in 3D point cloud
    - remove dynamic objects in 3D point cloud to obtain M<sub>s</sub>
    - M<sub>s</sub> represents a cleaner sparse semenatic 3D map
  
  - semantic score
    - obtain top-k ranked database images I<sub>R</sub> for each query image I<sub>Q</sub> by `NetVLAD`
    - for every selected I<sup>i</sup><sub>R</sub>, find 2D-3D matches through KNN search and ratio test (blue dotted lines)
    - obtain 2D-3D correpsondences between I<sup>i</sup><sub>R</sub> and M<sub>s</sub> (green solid lines)
    - obtain 3D-2D matches between M<sub>s</sub> and I<sub>Q</sub> (red solid lines)
    - apply `PnP solver`to recover query pose
    - project all `visiable` 3D points into I<sub>Q</sub> (`visiable` means 3D points should be seen by I<sub>Q</sub>)
    - count # 3D points whose semenatic labels are the same as those in I<sub>Q</sub>
  
  - weighted RANSAC pose estimation
    - 2D-3D matches produced by the same I<sup>i</sup><sub>R</sub> are assigned the semantic score of I<sup>i</sup><sub>R</sub> 
    - normalize each score by the sum of all 2D-3D match socres
    - use the normalized score as a `weight p for RANSAC's sampling` 
    - different from removing 2D-3D matches with lower semantic scores
 
  - appendix
    - DeepLabv3+ network ([paper link](http://openaccess.thecvf.com/content_ECCV_2018/html/Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper.html))
    - NetVLAD ([paper link](http://openaccess.thecvf.com/content_cvpr_2016/html/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.html))
    
---

<p align="center">
<img src="../img/20190829FeatureMatching.png" width="80%" hegiht="80%"/>
</p>

## Experiment
- visual localization dataset RobotCar Seasons ([Benchamrk](http://openaccess.thecvf.com/content_cvpr_2018/html/Sattler_Benchmarking_6DOF_Outdoor_CVPR_2018_paper.html))
- use `DeepLabv3+network` to segment all dataset images and assign a label to each 3D point by maximum voting with reprojecting pixel labels in all its visiable database images
- in the image retrival step, use `NetVLAD` and `pre-trained Pitts30K model` generate `4096-dimensional descriptor vectors` for each query and database images, and normalize L2 distances of the descriptors
- retrival number: k=30 for day conditions; k=50 for night conditions
- precision threshold: high (0.25m, 2deg); mdeium (0.5m, 5deg); coarse (5m, 10deg)
- comparsion with related works: Dense-VLAD, NetVLAD, FAB-MAP, Active Search, CSL, Non-semantic

---
