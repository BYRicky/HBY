---
layout: post
title: 'Summary for the paper "Robust Visual Localization in Changing Lighting Conditions"'
subtitle: 'Authors for the paper: Pyojin Kim, Brian Coltin, Oleg Alexandrov, H. Jin Kim'
tags: ["Visual Localization", "light changing condition"]
---

## Paper
<a href="https://ieeexplore.ieee.org/document/7989640">Robust Visual Localization in Changing Lighting Conditions</a>

---

## Goal
Provide an illumination-robust visual localization alogrithm for Astrobee* <br>
(* a free-flying robot designed to autonomously navigate on the International Space Station (ISS))

---

## Keykey points 
1. fetch the current `illumination level`
2. select `an appropriate map` <br>
(find the nearest brightness by symmetric KL-divergence value)
3. adjust `camera exposure time` <br>
(if the estimated lighting condition is too dark, <br>
 then increasing exposure time, else decreasing)
4. camera with `adjustable` exposure time
5. operate in the `fixed region`

---

## Astrobeeâ€™s Current Localization System
- **Offline Map Construction + Online Image Localization**
  - Offline Map Construction
    - why: fixed operating region + higher stability and accuracy than existing SLAM systems
    - how: the following four steps <br>
      1. **SfM (SURF)**
      2. **rebuilding Map with** `BRISK feautres`
         - tradeoff between (accurate & robust) and efficiency with respect to SURF features
            - less accurate and robust (critical for accurate offline map building) 
            - faster (critical for online localization)
      3. **registration to ISS Coordinate** 
         - the map is registered to the pre-defined ISS coordinate system
      4. **vocabulary Tree Database** 
         - constructed for fast retrieval of similar images for localization
  - Online Image Localization
     - how: the following four steps <br>
       1. **detect Features with BRISK**
       2. **vocabulary Tree DB Query**
       3. **P3P with RANSAC**
       4. **Pose-only Bundle Adjustment**
       
---


