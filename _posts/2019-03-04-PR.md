---
layout: post
title: 'Perceptron classifier'
tags: ["matlab"]
---
(To be continued....) <br>
Here is an approach to implement simple perception classifier. I will show you guys the entire steps for this implementation.

We choose the Iris dataset. This dataset contains 3 classes of 50 instances each, where each class refers to a type of Iris plant. The first two classes are linearly separable from each other, whereas the last class is not linearly separable. With the limitation of this classifier, we only take the first two classes. The following d steps...

## Step 1: Load dataset.<br>
In matlab, we use `textread` to load the dataset with text format. In this dataset, there are four attributes except for the class name and each attribute is separated by comma. Note that in `textread`, the second parameter is the data type of the dataset (In Iris dataset, the first four features are floating number and the last one is the string), and the fourth one is the delimiter used in the dataset. As the result, we can write the followiong code.
~~~
[atrb1,atrb2,atrb3,atrb4,className]=textread(filename,'%f%f%f%f%s','delimiter,',');
~~~

## Step 2: Split training data and testing data. <br>
Assume that we have 100 instances, and serve 90 samples as training data. The straightforward way is to take the first 90 instances for training and the rest ones for testing. However, it cannot gurantee that these two subsets have similar distributions. In matlab, there is a built-in function `cvpartition`; it gives an systemical way to split training and testing data.
~~~ 
cv=cvpartition(className,'KFold', k);
~~~

## Step 3: Feature space. <br>
Typically, each instance is represented by a feature vector(i.e. written as a row vector [atrb1, atrb2, atrb3, atrb4]), and 
all feature vectors span the feature space. Each feature is a dimension of the feature space, and therefore Iris dataset forms a 4-dimensional feature space. We reduce the 4D space into multiple 2D spaces for clear visualization. Following the steps below, we can generate the feature space.

1. use `subplot` to display multiple plots in one figure. 
2. choose each two features to form a 2D space.
3. use `gscatter` to create a scatter plot of 2D points, grouped by className.

~~~
figure(1);
Tuple={atrb1,atrb2,atrb3,atrb4}
Title={'Speal.Length','Sepal.Width','Petal.Length','Petal.Width'};
for i=1:atrb
  for j=1:atrb
    if(i==j); continue; end
    subplot(atrb,atrb,(i-1)*atrb+j);
    gscatter(Tuple(:,i),Tuple(:,j),className,'rg','.',8);
    xlabel(Title(i)); ylabel(Title(j));
    legend('off');
 end
end
~~~

## Step 4: Establish a peceptron classifier. <br>
We give the entire concept of the peceptron classifier as follows. <br>

### 1. What is the peceptron classifier? <br>
It is one of the linear classifier which has linear decision bounderies. A linear classifier provides multiple linear equations to divide the classes into several regions. <p>
Now, we focus on the two-class problem, that is, only one linear equation is generated. Here is an example. Assume the point (2, 2) belongs to class1 whereas the point (0, 0) belongs to class2 and there is a decision boundary 2x + 3y - 4 = 0. 

### 2. How to verify the correctness of this eqaution? <br>
The point (2, 2) gives the result 2 * 2 + 3 * 2 - 4 > 0 while the point (0, 0) gives the result 2 * 0 + 3 * 0 - 4 < 0. One can find that class1 get the result "> 0" and class2 gets "< 0" (i.e., they are on the different sides of the boundary). <p>
The coefficients 2, 3, and 4 (x, and y) can be viewed as the weight vector W [w0, w1, w2] (instances X [x0, x1]). As the result, the equation 2x + 3y - 4 = 0 can be rewritten as W·X<sup>T</sup>=0, where "·" stands for inner product. However, we want to user this equation to do evaluation, it cannot be assigned to zero. It should be: W·X<sup>T</sup> > 0 if X is in class1 and W·X<sup>T</sup> < 0 if X is in class2. <br>

### 3. X is known information, but how to find the suitable W? <br>
We start from the initial weight W=[0, 0, 0]. If this evaluation function W·X<sup>T</sup> gives the incorrect result, then W is updated by X. There are two situation, One is that the point in class1 is misclassified into class2, and the new weight vector is [0 + x0, 0 + x1, 0 + 1]. The other is that the point in class2 is then classified into class1, and the updated weight vector is [0 - x0, 0 - x1, 0 - 1]. Every misclassified instance leads to one update.

