---
layout: post
title: 'Perceptron classifier'
tags: ["matlab"]
---
(To be continued....) <br>
Here is an approach to implement simple perception classifier. I will show you guys the entire steps for this implementation.

We choose the Iris dataset. This dataset contains 3 classes of 50 instances each, where each class refers to a type of Iris plant. The first two classes are linearly separable from each other, whereas the last class is not linearly separable. With the limitation of this classifier, we only take the first two classes. The following d steps...

**Step 1: Load dataset.** <br>
In matlab, we use `textread` to load the dataset with text format. In this dataset, there are four attributes except for the class name and each attribute is separated by comma. Note that in `textread`, the second parameter is the data type of the dataset (In Iris dataset, the first four features are floating number and the last one is the string), and the fourth one is the delimiter used in the dataset. As the result, we can write the followiong code.
~~~
[atrb1,atrb2,atrb3,atrb4,className]=textread(filename,'%f%f%f%f%s','delimiter,',');
~~~

**Step 2: Split training data and testing data.** <br>
Assume that we have 100 instances, and serve 90 samples as training data. The straightforward way is to take the first 90 instances for training and the rest ones for testing. However, it cannot gurantee that these two subsets have similar distributions. In matlab, there is a built-in function `cvpartition`; it gives an systemical way to split training and testing data.
~~~ 
cv=cvpartition(className,'KFold', k);
~~~

**Step 3: Feature space.** <br>
Typically, each instance is represented by a feature vector(i.e. written as a row vector [atrb1, atrb2, atrb3, atrb4]), and 
all feature vectors span the feature space. Each feature is a dimension of the feature space, and therefore Iris dataset forms a 4-dimensional feature space. We reduce the 4D space into multiple 2D spaces for clear visualization. Following the steps below, we can generate the feature space.

1. use `subplot` to display multiple plots in one figure. 
2. choose each two features to form a 2D space.
3. use `gscatter` to create a scatter plot of 2D points, grouped by className.

~~~
figure(1);
Tuple={atrb1,atrb2,atrb3,atrb4}
Title={'Speal.Length','Sepal.Width','Petal.Length','Petal.Width'};
for i=1:atrb
  for j=1:atrb
    if(i==j); continue; end
    subplot(atrb,atrb,(i-1)*atrb+j);
    gscatter(Tuple(:,i),Tuple(:,j),className,'rg','.',8);
    xlabel(Title(i)); ylabel(Title(j));
    legend('off');
 end
end
~~~

**Step 4: Main algorithm of peceptron classifier.** <br>
What is a peceptron classifier? It is one of a linear classifier which has linear decision bounderies. A linear classifier provides multiple linear equations to divide the classes into several regions. Now, we focus on the two-class problem, that is, only one linear equation is generated. <br> <br>

Here is a examole. Assume the point (2,2) belongs to class1 whereas the point (0,0) belongs to class2 and there is a decision boundary 2x+3y-4=0. How to verify the correctness of this eqautuon? The point (2,2) gives the result 2*2+3*2-4>0 while the point (0,0) gives the result 2*0+3*0-4<0.

Think about the equation 2x+3y-4=0. The point (2,2) is on the right hand side of this equation because 4+6-4>0. Another point (0,0) is on the left hand side of the equation because 0+0-4<0. First we define class1 as '>0' whereas class2 defines as "<0'


